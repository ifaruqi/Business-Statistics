---
title: "Business Statistics End-Term Assessment IB94X0 2022-2023 #2"
author: '2247946'
date: "2022-12-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=FALSE}
knitr::include_graphics("Warwick_Business_School_logo.svg")
```


# Masters Programmes
## Assignment Cover Sheet

---


Submitted by        : 2247946 <br>
Date Sent           : 2023-01-16			
Module Title        : Business Statistics		
Module Code         : IB94X0		
Date/Year of Module : Term One 2022-2023<br>
Submission Deadline : Monday, 16th January 2023 (12:00 UK Time) 	
Word Count          :	Approximately (2500 words)
Number of Pages     : 1 HTML Pages		
Question            : Business Statistics (IB94X0) - Individual Assignment 2

<br>
“This is to certify that the work I am submitting is my own. All external references and sources are clearly acknowledged and identified within the contents. I am aware of the University of Warwick regulation concerning plagiarism and collusion.

No substantial part(s) of the work submitted here has also been submitted by me in other assessments for accredited courses of study, and I acknowledge that if this has been done an appropriate reduction in the mark I might otherwise have received will be made.”

<br>
“I declare that this work is entirely my own in accordance with the University's Regulation 11 and the WBS guidelines on plagiarism and collusion. All external references and sources are clearly acknowledged and identified within the contents. 

No substantial part(s) of the work submitted here has also been submitted by me in other assessments for accredited courses of study, and I acknowledge that if this has been done it may result in me being reported for self-plagiarism and an appropriate reduction in marks may be made when marking this piece of work.” 

---


# Question 1

# The Scenario

The dataset comes from the Food Standards Agency. A panel of Food Standard Agency board and local politicians want to better understand whether employing more professional enforcement officers increases the likelihood of establishments successfully responding to enforcement actions.

The Food Standards Agency (FSA) is responsible for monitoring and reporting on the performance of local authority (LA) food law enforcement services in England, Wales and Northern Ireland. Data are collected annually from Local Authorities, on food law enforcement activity within food establishments.

The dataset covers the period 1 April 2019 to 31 March 2020 so provides a picture of local authority activity at the point the UK-wide lockdown to control the spread of COVID-19 began in late March 2020.

Description of each variable in the dataset are shown below.

Variable | Description
------------- | -------------
Country | Country
LAType | LAType
LAName | LAName
Totalestablishments(includingnotyetrated&outside) | Number of Total establishments (including not yet rated & outside the programme)
Establishmentsnotyetratedforintervention | Number of Establishments not yet rated for intervention
Establishmentsoutsidetheprogramme | Establishments outside the programme
Total%ofBroadlyCompliantestablishmentsratedA-E | Total percentage of Broadly Compliant rated establishments
Total%ofBroadlyCompliantestablishments(includingnotyetrated) | Total percentage of Broadly Compliant rated establishments (including not yet rated establishments)
Aratedestablishments | Number of A-rated establishment
Total%ofBroadlyCompliantestablishments-A | Percentage of Broadly Compliant establishments rated A
Bratedestablishments | Number of B-rated establishment
Total%ofBroadlyCompliantestablishments-B | Percentage of Broadly Compliant establishments rated B
Cratedestablishments | Number of C-rated establishment
Total%ofBroadlyCompliantestablishments-C | Percentage of Broadly Compliant establishments rated C
Dratedestablishments | Number of D-rated establishment
Total%ofBroadlyCompliantestablishments-D | Percentage of Broadly Compliant establishments rated D
Eratedestablishments | Number of E-rated establishment
Total%ofBroadlyCompliantestablishments-E | Percentage of Broadly Compliant establishments rated E
Total%ofInterventionsachieved(premisesratedA-E) | Total percentage of Interventions achieved in rated establishments
Total%ofInterventionsachieved-premisesratedA | Percentage of Interventions achieved in establishments rated A
Total%ofInterventionsachieved-premisesratedB | Percentage of Interventions achieved in establishments rated B
Total%ofInterventionsachieved-premisesratedC | Percentage of Interventions achieved in establishments rated C
Total%ofInterventionsachieved-premisesratedD | Percentage of Interventions achieved in establishments rated D
Total%ofInterventionsachieved-premisesratedE | Percentage of Interventions achieved in establishments rated E
Total%ofInterventionsachieved-premisesnotyetrated | Percentage of Interventions achieved in establishments not yet rated
Totalnumberofestablishmentssubjecttoformalenforcementactions-Voluntaryclosure | Number of establishments subject to Voluntary closure
Totalnumberofestablishmentssubjecttoformalenforcementactions-Seizure,detention&surrenderoffood | Number of establishments subject to  Seizure,Detention & Surrender of Food
Totalnumberofestablishmentssubjecttoformalenforcementactions-Suspension/revocationofapprovalorlicence | Number of establishments subject to Suspension / Revocation of Approval or Licence
Totalnumberofestablishmentssubjecttoformalenforcementactions-Hygieneemergencyprohibitionnotice | Number of establishments subject to  Hygiene Emergency Prohibition Notice
Totalnumberofestablishmentssubjecttoformalenforcementactions-Prohibitionorder | Number of establishments subject to Prohibiton Order
Totalnumberofestablishmentssubjecttoformalenforcementactions-Simplecaution | Number of establishments subject to Simple Auction
Totalnumberofestablishmentssubjecttoformalenforcementactions-Hygieneimprovementnotices | Number of establishments subject to Hygiene Improvements Notices
Totalnumberofestablishmentssubjecttoformalenforcementactions-Remedialaction&detentionnotices | Number of establishments subject to Remedial Action & Detention Notices
TotalnumberofestablishmentssubjecttoWrittenwarnings | Number of establishments subject to Written Warnings
Totalnumberofestablishmentssubjecttoformalenforcementactions-Prosecutionsconcluded | | Number of establishments subject to Prosecutions Conculded
ProfessionalFullTimeEquivalentPosts-occupied* | Number of employees or professional enforcement officers


# The Request

This report fulfills the requests of the management board of Food Standards Agency, performing the specific analyses as follow:

  1. Distribution of successful enforcement actions across the Local Authorities (LAs) for 
      a. all establishments  
      b. each establishment ratings A, B, C, D, and E
  2. Examine whether there is a relationship between proportion of successful responses and the number of FTE food safety employees in each local authority.
  3. Examine whether there is a relationship between proportion of successful responses and the number of employees as a proportion of the number of establishments in the local authority

Examine whether employing more professional enforcement officers increases the likelihood of establishments successfully responding to enforcement actions.

# The Answer
The answer have two sections. 

The first section includes the code to perform all stages of the data analysis. This section could be shared with someone else who is not familiar with the data or project, but is an expert in R and statistics.

The second section is a polished and professional report presenting and interpreting the
findings for the panel of management board of Food Standards Agency and politicians.

## Section 1

```{r}
# Install and load the necessary package and library

#install.packages("tidyverse")
#install.packages("grid")
#install.packages("gridExtra")
#install.packages("kableExtra")
#install.packages("Rmisc")
#install.packages("emmeans")
# install.packages("ggpubr")
library(tidyverse)
library(tidyr)
library(lubridate)
library(grid)
library(gridtext)
library(gridExtra)
library(kableExtra)
library(dplyr)
library(ggpubr)
library(emmeans)
library(Hmisc)
library(car)

options(width=100)
```

### Data Preparation

```{r, message=FALSE}
# read and load the fire data from the London Fire Brigade
ratingdata <- read_csv("2019-20-enforcement-data-food-hygiene.csv")
```

```{r}
# check the summary of the data
summary(ratingdata)

# check the structure of the data
str(ratingdata)
```

After checking the data, it is found that there are 6 rows with empty values in which all of the column have N/A values. It means there is no data available for respective Local Authorities (LAs). 

It is also found that 'Total%ofBroadlyCompliantestablishments-A' and 'Total%ofInterventionsachieved-premisesratedA' have 'char' datatype. Both column are set to 'char' because there are several rows with character value indicating missing data. Finally, 'Country' and 'LAType' columns are also in 'char' datatype.

Data cleaning and preparation is required.

```{r}

# Change data types for Country and LAtype columns
# First generate a vector to keep the column names
columns_factor <- c("Country", "LAType")
columns_num <- c("Total%ofBroadlyCompliantestablishments-A", "Total%ofInterventionsachieved-premisesratedA")

# Then, set the correct data types for the defined columns
ratingdata[columns_factor] <- lapply(ratingdata[columns_factor], factor)

# To check the new structure of the data after several changes above
str(ratingdata)

```

Variable names in the dataset provided are self-explanatory in a sense that those names are the description of the respective variable. Hence, the variable names presented in the dataset are mostly too lengthy and may not be beneficial for the programming purposes. The following chunk is a dedicated chunk to record the variable name changes in this R markdown file.

```{r}
# Changing Column Name for Convenience
colnames(ratingdata)[which(names(ratingdata) == "Total%ofInterventionsachieved(premisesratedA-E)")] <- "OverallInterventionsAchieved"
colnames(ratingdata)[which(names(ratingdata) == "Total%ofInterventionsachieved-premisesratedA")] <- "ARatedInterventionsAchieved"
colnames(ratingdata)[which(names(ratingdata) == "Total%ofInterventionsachieved-premisesratedB")] <- "BRatedInterventionsAchieved"
colnames(ratingdata)[which(names(ratingdata) == "Total%ofInterventionsachieved-premisesratedC")] <- "CRatedInterventionsAchieved"
colnames(ratingdata)[which(names(ratingdata) == "Total%ofInterventionsachieved-premisesratedD")] <- "DRatedInterventionsAchieved"
colnames(ratingdata)[which(names(ratingdata) == "Total%ofInterventionsachieved-premisesratedE")] <- "ERatedInterventionsAchieved"
colnames(ratingdata)[which(names(ratingdata) == "ProfessionalFullTimeEquivalentPosts-occupied *")] <- "FTE"
```

```{r}
# Checking outlier in successful intervention rate
boxplot(ratingdata$OverallInterventionsAchieved)

# Checking outlier in successful intervention rate
boxplot(ratingdata$FTE)
```

Result: Data seems normal, the value are possible to be the real value (not input error). Conclusion: Keep all the data.

```{r}
# Create new data frame for further analysis & keep the original data
rtgdata <- ratingdata

# Change the "NR" and "NP" value to NA
rtgdata["ARatedInterventionsAchieved"][rtgdata["ARatedInterventionsAchieved"] == "NR"] <- NA
rtgdata["Total%ofBroadlyCompliantestablishments-A"][rtgdata["Total%ofBroadlyCompliantestablishments-A"] == "NP"] <- NA

# Convert from char to num data type
rtgdata$`Total%ofBroadlyCompliantestablishments-A` <- as.numeric(rtgdata$`Total%ofBroadlyCompliantestablishments-A`)
rtgdata$`ARatedInterventionsAchieved` <- as.numeric(rtgdata$`ARatedInterventionsAchieved`)

# create new column for analysis

# Number of rated establishment
rtgdata$ratedestablishment <- rtgdata$`Totalestablishments(includingnotyetrated&outside)`-rtgdata$Establishmentsnotyetratedforintervention - rtgdata$Establishmentsoutsidetheprogramme

rtgdata$FTEPerEstablishment <- 1000*rtgdata$FTE/rtgdata$ratedestablishment

rtgdata$ratedestablishmentin1000 <- rtgdata$ratedestablishment/1000

rtgdata$FTEAsProportion <- rtgdata$ratedestablishment*0.2
```

### 1a. Distribution of Percentage of Successful Enforcement Actions

```{r}
# Density Plot
dist_plot <- 
  ggplot(data = rtgdata)+
  geom_histogram(mapping = (aes(x=OverallInterventionsAchieved, y = ..density..)),binwidth = 5, color="black")+
  geom_density(aes(x=OverallInterventionsAchieved), fill = "red", alpha = 0.2)+
  geom_vline(aes(xintercept = mean(ratingdata$OverallInterventionsAchieved, na.rm = TRUE)), col = "red", size = 1)+
  geom_text(aes(x=mean(OverallInterventionsAchieved, na.rm = TRUE) - 10, 
                label=paste("Mean =", as.character(round(mean(OverallInterventionsAchieved, na.rm=TRUE))),"%"), 
                                           y=0.04), colour = "red")+
  ggtitle("Distribution of Successful Intervention Rate Accross Local Authorities")+
  labs(x = "% of Successful Interventions", y = "Density")+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(title = element_text(face="bold"))+
  theme(legend.position = "none")


# Frequency Plot
ggplot(data = rtgdata)+
  geom_histogram(mapping = (aes(x=OverallInterventionsAchieved)),binwidth = 5, color="black")+
  geom_vline(aes(xintercept = mean(ratingdata$OverallInterventionsAchieved, na.rm = TRUE)), col = "red", size = 1)+
  geom_text(aes(x=mean(OverallInterventionsAchieved, na.rm = TRUE) - 10, 
                label=paste("Mean =", as.character(round(mean(OverallInterventionsAchieved, na.rm=TRUE))),"%"), 
                                           y=80), colour = "red")+
  ggtitle("Distribution of Successful Intervention Actions Rate Local Authorities")+
  labs(x = "% of Successful Interventions", y = "Frequency")+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(title = element_text(face="bold"))+
  theme(legend.position = "none")

```

### 1b. Distribution of Percentage of Successful Enforcement Actions for Each Rating


```{r}
# Convert the dataset into long format

# Pick relevant columns
data_wide <- rtgdata[c("Country", "LAType", "LAName", "ARatedInterventionsAchieved","BRatedInterventionsAchieved","CRatedInterventionsAchieved","DRatedInterventionsAchieved","ERatedInterventionsAchieved", "FTE")]

data_wide$ARatedInterventionsAchieved <- as.numeric(data_wide$ARatedInterventionsAchieved)

# Convert the data from wide to long format
data_long <- data_wide %>% 
  pivot_longer(
    cols = `ARatedInterventionsAchieved`:`ERatedInterventionsAchieved`, 
    names_to = "rating",
    values_to = "interventionsuccessrate"
)

data_long$rating <- as.factor(data_long$rating)

str(data_wide)
```

```{r}
# Create a summary of Response Time data
mean_achieved <- data_long %>% 
  group_by(rating) %>% 
  summarise(successrate = mean(interventionsuccessrate, na.rm = TRUE))

# Create a new column to save the label for the vline
mean_achieved$'SuccessRateLabel' <- paste("Mean = ", as.character(round(mean_achieved$successrate)),"%")

# Create label for the plot
rating.labs <- c("ARatedInterventionsAchieved" = "Rating A" , "BRatedInterventionsAchieved" = "Rating B", "CRatedInterventionsAchieved" = "Rating C",
                 "DRatedInterventionsAchieved" = "Rating D", "ERatedInterventionsAchieved" = "Rating E")
```

```{r}
# Frequency Plot
rating_freqplot<- 
  ggplot(data = data_long)+
  geom_histogram(mapping = (aes(x=interventionsuccessrate, fill = rating)), color="black")+
  geom_vline(data = mean_achieved,aes(xintercept = successrate), col = "red", size = 1)+
  geom_text(data = mean_achieved,aes(x=25, label=SuccessRateLabel,
                                          y=c(150, 80, 80, 70, 40)),colour="black")+
  #ggtitle("Distribution of Successful Enforcement Actions for Each Rating")+
  labs(x = "% of Successful Interventions", y = "Frequency")+
  facet_grid(rating ~., labeller = labeller(rating = rating.labs), scales = "free")+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(title = element_text(face="bold"))+
  theme(legend.position = "none")

# Density Plot
rating_densplot <- 
  ggplot(data = data_long)+
  geom_histogram(mapping = (aes(x=interventionsuccessrate, y= ..density.., fill = rating)), color="black")+
  geom_density(aes(x=interventionsuccessrate), fill = "black", alpha = 0.3)+
  geom_vline(data = mean_achieved,aes(xintercept = successrate), col = "red", size = 1)+
  geom_text(data = mean_achieved,aes(x=25, label=SuccessRateLabel,
                                          y=c(0.15, 0.1, 0.075, 0.06, 0.03)),colour="black")+
  #ggtitle("Distribution of Successful Enforcement Actions for Each Rating")+
  labs(x = "% of Successful Interventions", y = "Density")+
  facet_grid(rating ~., labeller = labeller(rating = rating.labs), scales = "free")+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(title = element_text(face="bold"))+
  theme(legend.position = "none")

```

###   2. Examine whether there is a relationship between proportion of successful responses and the number of FTE food safety employees in each local authority.


```{r}
# Shapiro Test to Test Normality of The Data
shapiro.test(rtgdata$OverallInterventionsAchieved)
shapiro.test(rtgdata$FTE)
```
From the output, the p-value < 0.05 implying that the distribution of the data are significantly different from normal distribution. In other words, we can not assume the normality. 

Therefore, in checking correlation, it is more suitable to use 'spearman' method instead of pearson.

```{r}
# Checking the correlation between FTE of FSA and Percentage of Intervention Achieved
rcorr(as.matrix(select(rtgdata, OverallInterventionsAchieved, FTE, ratedestablishment)), type = "spearman")
```

The correlation between FTE and Overall Intervention Achieved is not significant under NHST (p-value = 0.9771, n = 347). 

We can see the relationship between FTE and Overall Intervention Achieved visually with the graph below:

```{r}

# FTE vs Overall Intervention Achieved
( allcountry <- 
  ggplot(rtgdata, aes(y=OverallInterventionsAchieved, x=FTE)) + 
  geom_point() + 
  labs(x="Number of FTE", y="% of Successful Intervention", subtitle = "All Countries") + 
  scale_y_continuous(breaks = c(0, 25, 50, 75, 100))+
  geom_smooth(method=lm)+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0)))

```

```{r}
# FTE vs Overall Intervention Achieved in Each Country
( eachcountry <- 
  ggplot(rtgdata, aes(y=OverallInterventionsAchieved, x=FTE, colour = Country)) + 
  geom_point() + 
  scale_y_continuous(limits =c(0,125), breaks =c(0,25,50,75,100))+
  facet_grid(Country ~.)+ 
  geom_smooth(method=lm)+
  labs(x = "Number of FTE", y = "% of Successful Interventions", subtitle = "Each Country")+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0))+
  theme(legend.position="none")
)

```

```{r}
# Remove axis titles from all plots
p = list(allcountry,eachcountry) %>% map(~.x + labs(x=NULL, y=NULL))

# Arrange the plot
grid.arrange(grobs=p, ncol = 2, top = text_grob("Number of FTE vs Successful Intervention", size = 12, face = "bold"), left = text_grob("% of Successful Interventions", rot = 90, face ="bold"), bottom = text_grob("Number of FTE", face = "bold"))
```

We can then try to build simple linear model to predict the Overall Intervention Achieved by the FTE.

```{r}
# Create a linear model to predict success rate based on the number of FTE
successrate_by_FTE <- lm(OverallInterventionsAchieved ~ FTE, data = rtgdata)
summary(successrate_by_FTE)
cbind(coefficient=coef(successrate_by_FTE), confint(successrate_by_FTE))
```

The model 'successrate_by_FTE' is a linear regression of the Overall Intervention Achieved (%) by the number of Full Time Employee (FTE).
The tells us that there is a 0.12% decrease of Overall Intervention Achieved for every increase of 1 FTE 95% CI [-0.64 - 0.4]. However, the result also shows that the FTE is not a significant predictor for Overall Intervention Achieved (t(345) = -0.447, p = 0.655, 95% CI [-0.64 - 0.4]).


```{r}
# Create variable to store data for each country
englanddata <- rtgdata %>% 
  filter (Country == "England")

northernirelanddata <- rtgdata %>% 
  filter (Country == "Northern Ireland")

walesdata <- rtgdata %>% 
  filter (Country == "Wales")
```

```{r}
# Create a linear model to predict success rate based on the number of FTE for each country

# England
successrate_by_FTE_England <- lm(OverallInterventionsAchieved ~ FTE, data = englanddata)
summary(successrate_by_FTE_England)
cbind(coefficient=coef(successrate_by_FTE_England), confint(successrate_by_FTE_England))

# Northern Ireland
successrate_by_FTE_NorthernIreland <- lm(OverallInterventionsAchieved ~ FTE, data = northernirelanddata)
summary(successrate_by_FTE_NorthernIreland)
cbind(coefficient=coef(successrate_by_FTE_NorthernIreland), confint(successrate_by_FTE_NorthernIreland))

# Wales
successrate_by_FTE_Wales <- lm(OverallInterventionsAchieved ~ FTE, data = walesdata)
summary(successrate_by_FTE_Wales)
cbind(coefficient=coef(successrate_by_FTE_Wales), confint(successrate_by_FTE_Wales))
```

The models of each country show that FTE is not a significant predictor for successful intervention rate. For England, the model shows that there is a $0.29$% decrease of successful intervention rate for every additional of 1 FTE ($t(312) = -0.975$, $p = 0.33$, 95% CI [$-0.89 - 0.3$]). In Northern Ireland, there is a $0.83$% increase of successful intervention rate for every increase of 1 FTE ($t(9) = 0.506$, $p = 0.625$, 95% CI [$-2.9 - 4.5$]). Finally, for Wales the model shows that there is a $0.2$% decrease of successful intervention rate for 1 additional FTE (($t(20) = -0.297$, $p = 0.769$, 95% CI [$-1.67 - 1.2$])).

###   3. Examine whether there is a relationship between proportion of successful responses and the number of employees as a proportion of the number of establishments in the local authority.


```{r}
# Shapiro-Wilk Test to test normality
shapiro.test(rtgdata$FTEPerEstablishment)
```
From the output, the p-value < 0.05 implying that the distribution of the data are significantly different from normal distribution. In other words, we can not assume the normality. 

Therefore, in checking correlation, it is more suitable to use 'spearman' method instead of pearson.

```{r}
# Checking the correlation between FTE of FSA and Percentage of Intervention Achieved
rcorr(as.matrix(select(rtgdata, OverallInterventionsAchieved, FTEPerEstablishment)), type = "spearman")
```

The correlation between the FTE per Total Rated Establishment and Overall Intervention Achieved is significant under NHST (p-value < 0.001, n = 347). This tells us that the r-value is not zero, but we can see that it is still small: r = 0.24 (5.8% shared variance).

We can see the relationship between FTE per Total Rated Establishment and Overall Intervention Achieved visually with the graph below:

```{r}
# Create a plot showing relationship between number of FTE per Total Rated Establishments and Overall Intervention Achieved
FTE_Per_Est_Plot <- 
  ggplot(rtgdata, aes(x = FTEPerEstablishment, y = OverallInterventionsAchieved)) +
  geom_point() +
   labs(x="FTE per Rated Establishment", y="% of Successful Intervention", 
       subtitle="Each point is a Local Authority \nThe shaded area shows the 95% CI for the best-fitting regression line") + 
  scale_y_continuous(limits =c(0,125), breaks = c(0, 25, 50, 75, 100))+
  geom_smooth(method=lm)+
  ggtitle("FTE Per Total Rated Establishment vs. Succesful Intervention")+
  labs(x = "FTE per Total Rated Establishment (x1,000)", y = "% of Successful Interventions")+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0))+
  theme(title = element_text(face="bold"))

```

We can then try to build simple linear model to predict the Overall Intervention Achieved by the FTE per Total Rated Establishment.

```{r}
# Create a linear model to predict success rate based on the number of FTE
successrate_by_FTE_per_RatedEstablishment <- lm(OverallInterventionsAchieved ~ FTEPerEstablishment, data = rtgdata)
summary(successrate_by_FTE_per_RatedEstablishment)
cbind(coefficient=coef(successrate_by_FTE_per_RatedEstablishment), confint(successrate_by_FTE_per_RatedEstablishment))
```

The model 'successrate_by_FTE_per_RatedEstablishment' is a linear regression of the Overall Intervention Achieved (%) by the proportion of FTE per Rated Establishment.
The model tells us that there is a 2.4% significant increase of Overall Intervention Achieved for every increase of 1,000 FTE Per Rated Establishment (t(345) = 3.9, p < 0.001, 95% CI [1.19 - 3.62]).

### Examine whether employing more professional enforcement officers increases the likelihood of establishments successfully responding to enforcement actions

To answer the question whether employing more professional enforcement officers increases the likelihood of establishments successfully responding to enforcement actions, it is interesting to also investigate the effect of both Number of FTE and Number of Rated Establishment within the Local Authorities. As demonstrated in the previous question, combining the value of FTE and Number of Rated Establishment as a proportion leads to significant predictor.

```{r}
# Create a linear model with FTE and Total Number of Rated Establishment as Predictors
successrate_by_FTE_and_RatedEstablishment <- lm(OverallInterventionsAchieved ~ FTE + ratedestablishmentin1000, data = rtgdata)
summary(successrate_by_FTE_and_RatedEstablishment)
cbind(coefficient=coef(successrate_by_FTE_and_RatedEstablishment), confint(successrate_by_FTE_and_RatedEstablishment))
```

As expected, both of FTE and Number of Rated Establishment are significant prediction for Overall Intervention Achieved. 

In the model involving both predictors, the successful intervention rate increase by a statistically significant $1.38$%, $t(344) = 3.13, p=0.0019$, for every additional of 1 FTE, holding the number of establishment constant.

On the contrary, the successful intervention rate decreases by a statistically significant $4.93$%, $t(344)= -4.22, p<.0001$, for every additional of 1,000 establishments, holding the number of FTE constant.


```{r}
anova(successrate_by_FTE, successrate_by_FTE_and_RatedEstablishment)
```
With anova test, it is concluded that adding number of rated establishments as independent variable significantly improves the overall model fit compared to the base model, in which only FTE is considered as the independent variable $F(1,344)= 17.83, p < .0001$.


```{r}
vif(successrate_by_FTE_and_RatedEstablishment)
```

Both VIF value are below 5. It is justified to keep both variable in the model.

```{r}
# Create a dataframe as the input to the model
predict_data <- expand.grid(FTE = seq(1, 20, 0.5),
                            ratedestablishmentin1000 =seq(3,10,0.5))

# Predict the intervention success rate using FTE and Number of Rated Establishment
predict_data <- mutate(predict_data,
                         OverallInterventionsAchieved = predict(successrate_by_FTE_and_RatedEstablishment, predict_data))

# Create dataframe for example
example_data <- expand.grid(FTE = c(7, 13),
                            ratedestablishmentin1000 = c(5,8))

example_data <- example_data[-c(4), ]

example_data$State <- c("Initial", "Scenario 1", "Scenario 2")

example_data[nrow(example_data) +1,] = c(20, 3, "Optimal")

example_data$FTE <- as.numeric(example_data$FTE)

example_data$ratedestablishmentin1000 <- as.numeric(example_data$ratedestablishmentin1000)
```



```{r}
# Create a plot showing the relationship between number of FTE and number of rated establishment as predictors for successful intervention  rate
cont_plot <-
ggplot()+
scale_color_viridis_c(option = "viridis")+
geom_point(data = predict_data, aes(x=FTE, y = ratedestablishmentin1000, color = OverallInterventionsAchieved), size = 15, shape = 15)+
ggtitle("FTE and Number of Rated Establishment vs Successful Intervention Rate")+
labs(x = "FTE", y = "Total Number of Rated Establishment (x1,000)", color = "% Successful Intervention")+
scale_x_continuous(limits = c(1,20), breaks = seq(2,20,2))+
scale_y_continuous(limits = c(3,10), breaks = seq(3,10,1))+
theme_classic()+
theme(plot.title = element_text(hjust = 0.5))+
theme(title = element_text(face="bold", size = 8))+
geom_point(data = example_data, aes(FTE, ratedestablishmentin1000, shape = State), size = 3)
```

---

## Section 2

### Introduction

This report elaborate data from The Food Standards Agency (FSA), which is responsible for monitoring and reporting on the performance of local authority (LA) food law enforcement services in England, Wales and Northern Ireland. Data are collected annually from all Local Authorities (LAs), on food law enforcement activity within food establishments. 

This report covers the period 1 April 2019 to 31 March 2020 so provides a picture of local authority activity at the point the UK-wide lockdown to control the spread of COVID-19 began in late March 2020.

There are 353 Local Authorities in the data sets used for this report. All but six Local Authorities returns were received in time for the data analysis for this report. There is no outlier identified and therefore all of the data are used in the report.

Particularly, this report will focus on whether establishments successfully respond to intervention actions and whether employing more professional enforcement officers increases the likelihood of establishments successfully responding to enforcement actions.

### 1. Distribution of Rate of Successful Interventions Accross Local Authorities

The distribution of Rate of Successful Intervention across local authorities is shown in the figure 1 below:

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.align='center'}
plot(dist_plot)
```
<center>**Figure 1. Distribution of Rate of Successful Intervention for All Establishment**</center>

As shown in Figure 1, the distribution of successful intervention rate for all local authorities is negatively skewed (skew to the right). It means that most of the local authorities has success rate closer to the upper bound (100%) rather than the lower bound (0%). For all of the local authorities, the average success rate is $87$%. This negative skewness is expected due to the nature of the measure. However, to tell whether the result of 87% is good or not, more context such as the government standard and past years result should be provided.

To understand better about the distribution of successful intervention rate, a closer look on how the successful intervention rate differs for each establishment rating is provided. Figure 2 below shows the successful intervention rate for each ratings:

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.align='center'}
# Plot the distribution of response time for each incident group
grid.arrange(rating_densplot, rating_freqplot, ncol = 2, 
             top = text_grob("Distribution of Successful Intervention Rate for Each Establishment Rating", size = 10, face = "bold"))
```
<center>**Figure 2. Distribution of Rate of Successful Intervention for Each Establishment Rating**</center>

Establishments within each local authority are rated for their potential impact on public health (essentially a measure of how many people could they cause sickness to, and how seriously, if they serve bad food). These ratings are letter grades from A – meaning the greatest potential impact – to E – the lowest potential impact upon public health.

Figure 2 shows that A-rated establishments has the highest successful intervention rate ($98$%). The distribution of successful intervention rate from A-rated establishments show that most of those establishments have $100$% success rate. Besides A-rated establishment, only B-rated and C-rated establishment have greater successful intervention rate than the overall establishment with $95$% and $92$% successful intervention rate respectively. D-rated establishments get a slightly below overall successful intervention rate with $86$% and E-rated establishments only get 77% successful intervention rate, or 10% below the overall rate. 

There is a noticeable trend that the establishments with high potential impact on public health will have greater successful intervention rate compared to the establishments with low impact. The trend is expected since establishments with high impact on public health must be ensured to have successful intervention rate as high as possible. To improve overall successful intervention rate, local authorities can improve the rate for E-rated establishments while keeping the successful intervention for high impact establishments high.

### 2. Relationship Between Successful Intervention Rate and The Number of Full Time Equivalent (FTE) of Food Standard Agency (FSA) Employees in Each Local Authority

In an effort to improve the successful intervention rate, Figure 3 below is prepared to examine the relationship between proportion of successful responses and the number of Full Time Equivalent (FTE) of Food Standard Agency (FSA) employees in each local authority.

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.align='center'}
grid.arrange(grobs=p, ncol = 2, top = text_grob("Number of FTE vs Successful Intervention Rate", size = 12, face = "bold"), left = text_grob("% of Successful Interventions", rot = 90, face ="bold"), bottom = text_grob("Number of FTE", face = "bold"))
```
<center>**Figure 3. Number of FTE vs Successful Intervention Rate**</center>

Figure 3 shows the relationship between FTE of Food Standard Agency (FSA) employees and the successful intervention rate. The graph on the right show the relationship for overall data including all three countries and three graph on the left shows the relationship in each of the country. At a quick glance, we cannot conclude that FTE has linear relationship with successful intervention rate.

To get a more complete understanding of the relationship between the two variables, a linear model is built. For overall data, the model shows that there is a $0.12$% decrease of successful intervention rate for every increase of 1 FTE $95$% CI [$-0.64 - 0.4$]. However, the result also shows that the FTE is not a significant predictor for successful intervention rate ($t(345) = -0.447$, $p = 0.655$, 95% CI [$-0.64 - 0.4$]).

The relationship between the two variables is also examined for each country. For England, the model shows that there is a $0.29$% decrease of successful intervention rate for every additional of 1 FTE ($t(312) = -0.975$, $p = 0.33$, 95% CI [$-0.89 - 0.3$]). In Northern Ireland, there is a $0.83$% increase of successful intervention rate for every increase of 1 FTE ($t(9) = 0.506$, $p = 0.625$, 95% CI [$-2.9 - 4.5$]). Finally, for Wales the model shows that there is a $0.2$% decrease of successful intervention rate for 1 additional FTE (($t(20) = -0.297$, $p = 0.769$, 95% CI [$-1.67 - 1.2$])). However, similar with the overall result, the result from each country shows that there is **no significant** relationship between number of FTE and the successful intervention rate in each country.

**Conclusion:** Number of FTE is not a significant predictor for successful intervention rate.

### 3. Relationship Between Successful Intervention Rate and The Number of Full Time Equivalent (FTE) of Food Standard Agency (FSA) Employees as a Proportion of Total Number of Rated Establishments in Each Local Authority

After concluding that number of FTE is not a significant predictor for successful intervention rate, another variable is studied. A linear model is built to predict successful intervention rate by number of FTE as a proportion of total number of rated establishments. 

The model shows that there is a 2.4% **significant** increase of successful intervention rate for every increase of 1,000 FTE as a proportion of total number of rated establishments (t(345) = 3.9, p < 0.001, 95% CI [1.19 - 3.62]).

Figure 4 below depicts the relationship between successful intervention rate and FTE as a proportion of total number of rated establishments.

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.align='center'}
plot(FTE_Per_Est_Plot)
```
<center>**Figure 4. Number of FTE per Total Number of Rated Establishment vs Successful Intervention Rate**</center>

As suggested from the model, with the increase of the proportion of FTE per total rated establishment, the successful intervention rate also increases.

**Conclusion:** Number of FTE alone is not a significant predictor for successful intervention rate. However, number of FTE as a proportion of total number of rated establishment is a significant predictor for  successful intervention rate

### Does Employing More Professional Enforcement Officers Increases The Likelihood of Establishments Successfully Responding to Intervention Actions?

The answer is : depend on the total number of rated establishment in the respective local authority, as suggested in the previous section. Another way to interpret section 3 is to build another model to predict successful intervention rate by FTE **AND** number of rated establishments as the predictors. As expected, both variable become significant predictors for successful intervention rate.

In the model involving both predictors, the successful intervention rate increase by a statistically significant $1.38$%, $t(344) = 3.13, p=0.0019$, for every additional of 1 FTE, holding the number of establishment constant.

On the contrary, the successful intervention rate decreases by a statistically significant $4.93$%, $t(344)= -4.22, p<.0001$, for every additional of 1,000 establishments, holding the number of FTE constant.

With anova test, it is concluded that adding number of rated establishments as independent variable significantly improves the overall model fit compared to the base model, in which only FTE is considered as the independent variable $F(1,344)= 17.83, p < .0001$.

Figure 5 illustrates the relationship between two independent variable and the target variable.

```{r,echo = FALSE, warning = FALSE, message = FALSE, fig.align='center'}
plot(cont_plot)
```
<center>**Figure 5. Number of FTE and Total Number of Rated Establishment vs Successful Intervention Rate**</center>

The top left corner of the graph in Figure 5 shows the condition where a local authority has low FTE and high number of establishments and as the model predict, the successful intervention rate is poor. On the opposite, in the bottom right corner shows the condition where a local authority has high FTE and low number of establishments and thus, the successful intervention rate is very high. 

Employing more officers (or increasing the number of FTE) will improve the successful intervention rate, assuming the number of establishments stays constant. This scenario is represented in Figure 5 by moving from the circle to the rectangle. Similarly, growing number of establishment in a local authority will lead to a decrease in successful intervention rate if the number of FTE stays the same (moving from the circle to the plus sign).

To achieve high successful intervention rate (represented by triangle), a local authority must have a proportion of approximately 20 FTE for every 3,000 establishments. 

**Conclusion:** Employing more Food Safety Agency employees in a local authority will improve the successful intervention rate, assuming the number of establishment within the respective local authority does  not change. A local authority can achieve high successful intervention rate by having a proportion of $1$ FTE for every $150$ establishments.

---

# Question 2

# The Scenario

The management team of a publishing company want to better understand factors that affect daily sales of books.

The data provided contains information on e-book sales over a period of many months. Each row in the data represents one book. The values of the variables are taken across the entire time period.

So daily.sales is the average number of sales (minus refunds) across all days in the period, and sale.price is the average price for which the book sold across all sales in the period.

Description of each variable in the dataset are shown below.

Variable | Description
------------- | -------------
sold.by | Name of the publisher
publisher.type | Classification of the publisher
genre | Genre of the book
avg.review | Average score of the book review
daily.sales | The average number of sales (minus refunds) across all days in the period
total.reviews | Total number of review for the book
sales.price | The average price for which the book sold across all sales in the period (assumption: in GBP)


# The Request

The management team would like you to provide a report that addresses the following: 

  1. Do books from different genres have different daily sales on average?
  2. Do books have more/fewer sales depending upon their average review scores and total number of reviews.
  3. What is the effect of sale price upon the number of sales, and is this different across genres?


# The Answer
The answer have two sections. 

The first section includes the code to perform all stages of the data analysis. This section could be shared with someone else who is not familiar with the data or project, but is an expert in R and statistics.

The second section is a polished and professional report presenting and interpreting the
findings for the panel of management board of the publishing company.

## Section 1


### Data Preparation
```{r}
# Read the dataset
salesdata <- read.csv("publisher_sales.csv")
```

```{r}
# Check the summary and the structure of the data
summary(salesdata)
str(salesdata)
```

```{r}

# Change data types for Country and LAtype columns
# First generate a vector to keep the column names
columns <- c("sold.by", "publisher.type", "genre")

# Then, set the correct data types for the defined columns
salesdata[columns] <- lapply(salesdata[columns], factor)

# To check the new structure of the data after several changes above
str(salesdata)

```

```{r}
# Check the correlation between variables
rcorr(as.matrix(select(salesdata, avg.review, daily.sales, total.reviews, sale.price)))
```

```{r}
# Checking outlier in avg.review
boxplot(salesdata$avg.review)
salesdata %>% filter (avg.review < 3)
# Result: Data seems normal, the value are possible to be the real value (not input error). Conclusion: Keep all the data.

# Checking outlier in daily.sales
boxplot(salesdata$daily.sales)
salesdata %>% filter (daily.sales > 175)
# Result: Data seems normal, the value are possible to be the real value (not input error). Conclusion: Keep all the data.

# Checking outlier in total.reviews
boxplot(salesdata$total.reviews)
salesdata %>% filter (total.reviews == 0)
# Result: Data seems normal, the value are possible to be the real value (not input error). Conclusion: Keep all the data.

# Checking outlier in sale.price
boxplot(salesdata$sale.price)
salesdata %>% filter (sale.price > 15)
# Result: Data seems normal, the value are possible to be the real value (not input error). Conclusion: Keep all the data.

```


### 1. Do books from different genres have different daily sales on average?

```{r}
# Plot to see the distribution of average daily sales for each genre
ggplot(salesdata, aes(daily.sales, ..density.., fill=genre))+
  geom_histogram(binwidth=10,position="identity", alpha=0.5)+ 
  labs(x="Average of Daily Sales", y="Density", fill="Genre")+
  ggtitle("Distribution of Average Daily Sales for Each Genre")+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(title = element_text(face="bold"))
```

```{r}
# Change the level in genre column to get positive result in mean difference (to be more intuitive)
salesdata$genre <- factor(salesdata$genre, levels = c("fiction", "non_fiction", "childrens"), labels = c("Fiction","Non Fiction", "Children"))

# Create subset data for t-test approach
fiction_childerns <- salesdata %>% 
  filter(genre %in% c("Fiction", "Children"))

fiction_nonfiction <- salesdata %>% 
  filter(genre %in% c("Fiction", "Non Fiction"))

childrens_nonfiction <- salesdata %>% 
  filter(genre %in% c("Children", "Non Fiction"))
```


```{r}
# Calculate simple statistical summary
salesdata_summary <- salesdata %>% 
 	group_by(genre) %>% 
	dplyr::summarise("Average of Daily Sales" = round(mean(daily.sales, na.rm = TRUE),1), 
	          "Standard Deviation of Average Daily Sales" = round(sd(daily.sales, na.rm=TRUE),1), 
	          "Frequency" = n()) 

# Change column name
colnames(salesdata_summary)[which(names(salesdata_summary) == "genre")] <- "Genre"

salesdata_summary

```


```{r}
# Two Sample t-test
t.test(daily.sales ~ genre, data = fiction_childerns)
t.test(daily.sales ~ genre, data = fiction_nonfiction)
t.test(daily.sales ~ genre, data = childrens_nonfiction)

```

Note that Welch Two Sample t-test is used. P-value obtained from each of the t-test suggest that null hypothesis can be rejected and therefore there are significant differences of average daily sales between each book genre.

```{r}
# Create a model to predict daily sales by genre
sales_by_genre <- lm(daily.sales ~ genre, data = salesdata)
summary(sales_by_genre)

# Estimation Approach
(sales_by_genre_emm <- emmeans(sales_by_genre, ~genre))
(sales_by_genre_con <- confint(pairs(sales_by_genre_emm))) 
```

```{r}
# Estimation of Average Daily Sales for Each Book Genre
estplot <- 
  ggplot(summary(sales_by_genre_emm), aes(x=genre, y=emmean, ymin=lower.CL, ymax=upper.CL)) +
  geom_point() + 
  geom_linerange() + 
  labs(y="Average of Daily Sales", x="Genre", subtitle="Error bars are 95% CIs",
		     title="Average Daily Sales for Each Book Genre")+
  scale_y_continuous(breaks = c(105.9, 75.9, 55.6))+
  theme(title = element_text(face="bold", size = 7))

# Difference of Average Daily Sales for Each Book Genre
conplot <-
  ggplot(sales_by_genre_con, aes(x=contrast, y=estimate, ymin=lower.CL, ymax=upper.CL)) +
  geom_point() + 
  geom_linerange() + 
  labs(y="Difference of Average Daily Sales", x="Genre", subtitle="Error bars are 95% CIs",
       title="Difference of Average Daily Sales for Each Book Genre") + 
  scale_y_continuous(breaks = c(50.3, 30, 20.3, 0))+
  geom_hline(yintercept=0, lty=2)+
  theme(title = element_text(face="bold", size = 7)) + 
  scale_x_discrete(labels = c("Fiction - \nChildren", "Fiction -\nNon Fiction", "Non Fiction - \nChildren"))

# Arrange both plot into a single plot
grid.arrange(estplot, conplot, ncol = 2)
```

Fiction genre has the highest average daily sales, followed by non fiction and children genre. The average daily sales for genre fiction is $105.9$ $95%$ CI $[104.9-106.9]$. The average daily sales for genre non fiction is $75.9$ $95%$ CI $[74.9-76.8]$. Finally, the average daily sales for genre children is $55.6$ $95%$ CI $[54.6-56.6]$.

The average daily sales of fiction genre is $30$ $95%$ CI $[28.4-31.7]$ and $50.3$ CI $[48.7-52]$ higher compared to non fiction and children genre respectively. Furthermore, the average daily sales of non fiction genre is $20.3$ $95%$ CI $[18.6-21.9]$ higher than children genre. 

The estimated difference along with its 95% confidence interval indicates that we can be sure that there is a difference in average daily sales between each genre. The fact that the lower bound of the confidence interval of the graph on the right does not intersect $y = 0$ line means that with 95% CI, the mean difference of average daily sales for each book genre is not equal to zero but rather falls in between its respective 95% confidence interval.


### 2. Do books have more/fewer sales depending upon their average review scores and total number of reviews.

```{r}
# Create a model to predict daily sales by average review score
sales_by_score <- lm(daily.sales ~ avg.review, salesdata)
summary(sales_by_score)
cbind(coefficient=coef(sales_by_score), confint(sales_by_score))
anova(sales_by_score)
```

```{r}
# Create a model to predict daily sales by total number of review
sales_by_totalreview <- lm(daily.sales ~ total.reviews, salesdata)
summary(sales_by_totalreview)
cbind(coefficient=coef(sales_by_totalreview), confint(sales_by_totalreview))
anova(sales_by_totalreview)
```

```{r}
# Predict total sales by average review scores and total number of review
sales_by_score_totalreview <- lm(daily.sales ~ avg.review + total.reviews, salesdata)
summary(sales_by_score_totalreview)
cbind(coefficient=coef(sales_by_score_totalreview), confint(sales_by_score_totalreview))
anova(sales_by_score_totalreview)
```

```{r}
# Check multicolinearity between the predictors
vif(sales_by_score_totalreview)
```
Both Variance Inflation Factor (VIF) scores are less then 5. It means that it is justified to keep both variable as the predictors for our model.

```{r}
# Examine model with interaction terms
sales_by_score_totalreview_int <- lm(daily.sales ~total.reviews * avg.review, salesdata)
summary(sales_by_score_totalreview_int)
cbind(coefficient=coef(sales_by_score_totalreview_int), confint(sales_by_score_totalreview_int))
anova(sales_by_score_totalreview_int)
```

```{r}
# Compare model with only main effect and the model with interaction terms
anova(sales_by_score_totalreview, sales_by_score_totalreview_int)
```

```{r}
# Visualisation of base model and model with interactions
surf_data <- tibble(total.reviews = unlist(expand.grid(seq(100, 250 , 25), seq(4, 5, 0.1))[1]),
                         avg.review = unlist(expand.grid(seq(100, 250, 25), seq(4, 5, 0.1))[2]))

surf_data <- mutate(surf_data,
                         main_hat = predict(sales_by_score_totalreview, surf_data),
                         intr_hat = predict(sales_by_score_totalreview_int, surf_data))

surf_main <- ggplot(surf_data, aes(total.reviews, avg.review)) + geom_contour_filled(aes(z = main_hat)) + labs(subtitle = "Main Effects")  + guides(fill=guide_legend(title="Average of Daily Sales"))
surf_intr <- ggplot(surf_data, aes(total.reviews, avg.review)) + geom_contour_filled(aes(z = intr_hat)) + labs(subtitle = "Interaction Effects")   + guides(fill=guide_legend(title="Average of Daily Sales"))
grid.arrange(surf_main, surf_intr, nrow = 2)
```

```{r}
# Create a plot to visualize the effect of interaction terms

score_review <- filter(surf_data, total.reviews %in% c(100, 125, 150, 175, 200, 200, 250)) %>%
  mutate(total.reviews = factor(total.reviews))
(
  predplot<-
  ggplot(score_review) + 
  geom_line(aes(avg.review, main_hat, colour = total.reviews), size = 1) +
  geom_line(aes(avg.review, intr_hat, colour = total.reviews), linetype = "dashed", size = 1) +
  labs(x = "Average of Review Score", y = "Prediction of Average Daily Sales")+
  ggtitle("Prediction of Average Daily Sales")+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(title = element_text(face="bold"))+
  theme(title = element_text(face="bold", size = 10)) +
  guides(colour=guide_legend(title="Total Number of Review"))
)

```

In the main effects model, the slope of average of review score against average of daily sales is always parallel for all different values of total number of review.

In the interaction model, the slope of average of review score against average of daily sales are different with different values of total number of review. With low number of review (< 150 review), the slope of average of review score against average of daily sales is negative. However, the more the review, the slope of average of review score against average of daily sales shifts to have steeper positive gradient. 

### 3. What is the effect of sale price upon the number of sales, and is this different across genres?

```{r}
# Create a model to predict average daily sales by average sale price
sales_by_price <- lm(daily.sales~sale.price, salesdata)
summary(sales_by_price)
cbind(coefficient=coef(sales_by_price), confint(sales_by_price))
anova(sales_by_price)
```

The results suggest that there is a decrease of 3.82 average daily sales for every 1 GBP increase of the average sale price. The confidence intervals does not include zero (95% CI = [-4.15, -3.48]) and thus, this decrease is significantly different from zero, $t(5998)=-22.38, p<0.0001$. We can conclude that relationship between average daily sales and total number of review is significant in a linear model.

```{r}
# Estimation Approach
( sales_by_price_emm <- emmeans(sales_by_price, ~sale.price, at=list(sale.price=c(1, 5, 9, 13, 17))))
```

```{r}
# Create a model to predict average daily sales by genre and sale price (with only main effect)
sales_by_price_genre <- lm(daily.sales~sale.price+genre, salesdata)
summary(sales_by_price_genre)
cbind(coefficient=coef(sales_by_price_genre), confint(sales_by_price_genre))
anova(sales_by_price_genre)
```

In the model involving both predictors, the average of daily sales increases by a statistically significant $0.54$, $t(5997) = 69.45, p<.0001$, for every extra review, holding the average of review score constant.

Similarly, the average of daily sales decreases by a statistically significant $3.94$, $t(5997)= -7.69, p<.0001$, for every increase of average review score by $1$ point, holding the number of total reviews constant.

```{r}
# Create a model to predict average daily sales by genre and sale price (with interaction terms)
sales_by_price_genre_int <- lm(daily.sales~sale.price*genre, salesdata)
summary(sales_by_price_genre_int)
cbind(coefficient=coef(sales_by_price_genre_int), confint(sales_by_price_genre_int))
anova(sales_by_price_genre_int)
```

The effect of genre varies significantly over sale price, $F(2, 5994)=10.32$, $p < 0.0001$.

```{r}
# Compare the model with only main effect and with the model with interaction terms
anova(sales_by_price, sales_by_price_genre_int)
```

Anova test result for the model shows that adding genre creates a better model than one containing only average of sales price, and that a model containing the interaction term is better than a model containing only the two main effects.

```{r}
# Estimate the value of average daily sales
( sales_by_price_genre_int_emm <- emmeans(sales_by_price_genre_int, ~sale.price+genre, at=list(sale.price=c(1, 5, 9, 13, 17))))

```


```{r}
# Combine model with only sale price with the model with sale price and genre
both_models_emms <- bind_rows(list(data.frame(sales_by_price_emm, Model="Only Sale Price"), data.frame(sales_by_price_genre_int_emm, Model="Sale Price and Genre")))

both_models_emms$genre <- as.character(both_models_emms$genre)

both_models_emms[is.na(both_models_emms)] = "Overall"

both_models_emms$genre <- as.factor(both_models_emms$genre)

both_models_emms$genre
```


```{r}
# Create a plot to visualize the effect of sale price to average daily sale in each genre
bothplot <- 
  ggplot(both_models_emms, aes(x=sale.price, y=emmean, ymin=lower.CL, ymax=upper.CL, col=genre)) + 
  geom_line(aes(x=sale.price, y= emmean, lty = Model)) + 
  geom_ribbon(aes(ymin = lower.CL, ymax = upper.CL, col = genre), alpha = 0.1)+
  labs(x="Sale Price (£)", y="Average of Daily Sales", col="Genre", subtitle="Error bars are 95% CIs")+
  ggtitle("Effect of Sale Price and Genre to Average Daily Sales")+
  scale_x_continuous(breaks = c(1, 5, 9, 13, 17))+
  scale_y_continuous(breaks = c(30, 60, 90, 120))+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0))+
  theme(title = element_text(face="bold"))+
  theme(title = element_text(face="bold", size = 10)) 

```

---

## Section 2

### Introduction 

This report will focus on examining the effect of genre, average of review score, total number of review, and average sales price on the average of daily sales.

The data provided contains information on e-book sales over a period of many months. There are 6,000 data entry in total, and each row in the data represents one book. The books included in this report come from three genres: Children, Fiction, and Non-Fiction. Each genre has the equal number of observations. There is no outlier identified with the data set and thus, all of the data are used for the analysis of this report. 

### 1. Relationship between Average Daily Sales and Genre

The average of daily sales for each book genre (fiction, non fiction, and children) are compared to see whether the books from different genres have different average of daily sales.

A table comparing the average of daily sales for each genre is prepared below.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
salesdata_summary %>%
  kbl(caption = "Table 1. Average of Daily Sales for Each Book Genre") %>%
  kable_styling()
```

Based on the result above, Null Hypothesis Significance Testing (NHST) and Estimation Approach are used to test the significance of the difference between the average of daily sales for each book genre.

**Null Hypothesis Significance Testing (NHST)**

In NHST method, t-test is used. A t-test is a statistical test that compares the means of two samples. In a two sample t-test, the null hypothesis is the means difference between two samples equal to zero. On the opposite, the alternate hypothesis is that the means difference between two sample is not equal to zero.

A two sample t-test was performed to compare average of daily sales (unit) between genres. The result show that there are significant difference of average daily sales between fiction and non fiction genre  $t(3532) = 37.78$, $p<0.0001$, fiction and children genre $t(2962) = 68.48$, $p<0.0001$ and non fiction and children genre $t(3687) = 36.35$, $p<0.0001$.

The result of the t-test shows that the $p-value$ is less then $0.005$ ($p-value < 0.005$), therefore Null Hypothesis can be rejected. It can be concluded that there is a significant difference in average daily sales between fiction $(Mean = 105.9, SD = 29.3)$, non fiction $(Mean = 75.9, SD = 20)$ and children $(Mean = 55.6, SD = 14.9)$ genre.

**Estimation Approach**

Rather than using one value to determine the significance difference of two means, estimation method introduce the concept of Confidence Interval (CI) to supplement the point estimate (in this case average of daily sales) with information about the uncertainty in this estimate.

After performing estimation approach, the result is obtained. Fiction genre has the highest average daily sales, followed by non fiction and children genre. The average daily sales for genre fiction is $105.9$ $95%$ CI $[104.9-106.9]$. The average daily sales for genre non fiction is $75.9$ $95%$ CI $[74.9-76.8]$. Finally, the average daily sales for genre childrens is $55.6$ $95%$ CI $[54.6-56.6]$.

The average daily sales of fiction genre is $30$ $95%$ CI $[28.4-31.7]$ and $50.3$ CI $[48.7-52]$ higher compared to non fiction and children genre respectively. Furthermore, the average daily sales of non fiction genre is $20.3$ $95%$ CI $[18.6-21.9]$ higher than children genre. 

The result can be incorporated in a graph as follow.

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.align='center'}
grid.arrange(estplot, conplot, ncol = 2)
```
<center>**Figure 1. Estimation of Average Daily Sales for Each Book Genre**</center>

The estimated difference along with its 95% confidence interval indicates that we can be sure that there is a difference in average daily sales between each genre. The fact that the lower bound of the confidence interval of the graph on the right does not intersect $y = 0$ line means that with 95% CI, the mean difference of average daily sales for each book genre is not equal to zero but rather falls in between its respective 95% confidence interval.

### 2.Effect of Average of Review Score and Total Number of Reviews On Average Daily Sales

Before examining the effect of average of review score and total number of review on average daily sales, simple linear models were built to understand whether Average Review Score or Total Number of Review is a significant predictor for Average Daily Sales. The models show that that relationship between average daily sales and average review score is not significant in a linear model $t(5998)=−0.32, p=0.75$. On the contrary, the results suggest that there is a significant average increase of $0.54$ average daily sales for every $1$ increase of total number of review $t(5998)=68.69, p<0.0001$. Therefore, the relationship between average daily sales and total number of review is significant in a linear model. 

To examine the effect of average review score and total number of review to average of book sales, two (2) linear models are examined. Those model are:

**a. Main Model**

In this model, both predictors (average of review score and total number of review) are used to predict average of daily sales.

In the model involving both predictors, the average of daily sales increases by a statistically significant $0.54$, $t(5997) = 69.45, p<.0001$, for every extra review, holding the average of review score constant.

Similarly, the average of daily sales decreases by a statistically significant $3.94$, $t(5997)= -7.69, p<.0001$, for every increase of average review score by $1$ point, holding the number of total reviews constant.

It is interesting to note that the average of review score become a significant predictor of average of daily sales when the effect of total review is held constant, but not when average of review score is the only predictor.

**b. Interaction Model**

In this model, both predictors (average of review score and total number of review) are used to predict average of daily sales. On top of that, interaction term between both prediction is also considered in the model.

The result shows that there is a significant positive interaction $(t(5996) = 11.41, p < 0.001)$ between total number of review and average score of the review when predicting average of daily sales. This means that when the value of total review is higher, the slope of average of review score is steeper.

With anova test, it is concluded that adding interaction terms between main effect (average of review score and total number of review) significantly improves the overall model fit compared to the main model, in which only main effect of both predictors are considered $F(1,5996)= 130.21, p < .0001$.

Interaction Model has the best overall model fit to predict average of daily sales. Therefore, the model is used to explain the relationship between dependent variable (average of daily sales) and independent variables (average of review score and total number of review). To illustrate the relationship, a graph is prepared below.

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.align='center'}
plot(predplot)
```
<center>**Figure 2. Prediction of Average Daily Sales from Average Review Score and Total Review**</center>

In **Figure 2.** above, solid line represent the prediction result from main model while the dashed line represent the prediction from the interaction model.

In the main model, the slope of average of review score against average of daily sales is always parallel for all different values of total number of review. It means that high average of review score always negatively impact the average of daily sales. The higher the average of review score, the lower the average of daily sales.

In the interaction model, the slope of average of review score against average of daily sales are different with different values of total number of review. With low number of review (< 150 reviews), the slope of average of review score against average of daily sales is negative. It means that The higher the average of review score, the lower the average of daily sales. However, the more the total number of review, the slope of average of review score against average of daily sales shifts to have steeper positive gradient. With high number of review (> 175 reviews), higher average of review score will lead to higher average of daily sales.

### 3. Effect of Sale Price On The Number of Overall Sales and Across Book Genres

**a. Effect of Sale Price On The Number of Sales**

To examine the effect of sale price to the number of sales, a linear model to predict number of sales (dependent variable) from sale price (independent variable) is created. The results suggest that there is a decrease of $3.82$ average daily sales for every $1$ GBP increase of the average sale price. The confidence intervals does not include zero $(95% CI = [-4.15, -3.48])$ and thus, this decrease is significantly different from zero, $t(5998)=-22.38, p<0.0001$. We can conclude that relationship between average daily sales and total number of review is _significant_ in a linear model.

**b. Effect of Sale Price On The Number of Sales for Different Genres**

Two check whether the effect of sale price on the number of sales differ across different genres, the variable genre is introduced to the model. On top of that, interaction term between both prediction is also considered in the model.
 
The result shows that there is a significant positive interaction $(t(5994) = 10.32, p < 0.001)$ between average of sale price and genre when predicting average of daily sales.

With anova test, it is concluded that adding genre as independent variable, along with interaction terms between genre and average of sale price significantly improves the overall model fit compared to the base model, in which only average of sale price is considered as the independent variable $F(4,5994)= 1102.7, p < .0001$.

To better illustrate the effect of average sale price and genre to the average of daily sales, a figure is prepared below.

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.align='center'}
plot(bothplot)
```
<center>**Figure 3. Effect of Sale Price and Genre to Average Daily Sales**</center>

In general, an increase on the sale price will reduce the average of daily sales. The trend applies to overall sale (represented by solid, purple line) and sale for each genre (represented by dashed line). The grey shade represents the 95% confidence interval for each genre and overall data.

However, further analysis shows that the effect of sale price is significantly differ across genre. As **Figure 3.** suggests, children genre is the most price sensitive among the other genres, indicated by a steep negative slope (red, dashed line). It means with the same increase of sale price, the sale of children genre will drop more than two other genres.


---

